import keyword
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
import math as ma
#import the dataset
iris=pd.read_csv('Iris.csv')
iris

# checkinf for null values if any#
iris.isna()

#describing statistical values#
iris.describe()

iris["Species"].value_counts()

#Data visualizations
iris.plot(kind="scatter",x="SepalLengthCm",y="SepalWidthCm")

sns.heatmap(iris.corr(),annot=True,cmap="Spectral")
plt.show()

sns.FacetGrid(iris,hue="Species", size=5)\
.map(plt.scatter,"SepalLengthCm","SepalWidthCm")\
.add_legend()



sns.FacetGrid(iris,hue="Species", size=5)\
.map(plt.scatter,"PetalLengthCm","PetalWidthCm")\
.add_legend()


#Elbow to find the optimum number of cluster:
x=iris.iloc[:,[0,1,2,3]].values
from sklearn.cluster import KMeans
WCSS=[]

for i in range(1, 11):
    kmeans = KMeans(n_clusters = i, init = 'k-means++',
		    max_iter = 300, n_init = 10,random_state = 0)
    kmeans.fit(x)
    WCSS.append(kmeans.inertia_)
#Plotting the result onto a line graph,
# Allowing us to observe 'The elbow'
plt.plot(range(1, 11), WCSS)
plt.title('The elbow method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS') #Within cluster sum of square
plt.show



kmeans = KMeans(n_clusters = i, init = 'k-means++',
      		    max_iter = 300, n_init = 10,random_state = 0)
y_kmeans=kmeans.fit_predict(x)

#Visualizing the cluster
plt.scatter(x[y_kmeans == 0, 0], x[y_kmeans == 0, 1],
            s=100, c = 'red', label = "Iris-setosa")
plt.scatter(x[y_kmeans == 1, 0], x[y_kmeans == 1, 1],
            s=100, c = 'green', label = "Iris-versicolour")
plt.scatter(x[y_kmeans == 2, 0], x[y_kmeans == 2, 1],
            s=100, c = 'blue', label = "Iris-verginica")
#Plotting the centroids of the clusters
plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1],
            s=100,c = 'black', label = 'Centroids')
plt.legend()

